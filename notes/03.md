# Optimizing an OpenCL Program

Actually, after learning the previous two parts, you have already exhausted the most of the OpenCL features, and you can write many parallel programs. However, you may make tons of mistakes that make your program run slower. This chapter introduce common mistakes and how to fix them. Optimization is usually related to the hardware details.

## Matrix Multiplication

Most of them can be demonstrated with a simple case- matrix multiplication. We first write the simplest program,

```rust
extern crate ocl;
use ocl::{ProQue};
use std::time::Instant;

const src_1: &str = r#"
__kernel void matrix_multiply(__global float* A, __global float* B, __global float* C, const unsigned int N) {
    int row = get_global_id(0);
    int col = get_global_id(1);
    for (int i = 0; i < N; i++) {
        C[row * N + col] += A[row * N + i] * B[i * N + col];
    }
}
"#;

fn task() -> ocl::Result<()> {

    let m_n = 2048;
    let matrix_size = m_n * m_n;

    let mut m_a = vec![0.0f32; matrix_size];
    let mut m_b = vec![0.0f32; matrix_size];

    for i in 0..m_n {
        for j in 0..m_n {
            m_a[i * m_n + j] = (i as f32) + (j as f32);
            m_b[i * m_n + j] = (i as f32) - (j as f32);
        }
    }

    let pro_que = ProQue::builder()
        .src(src_1)
        .dims([m_n, m_n])
        .build()?;

    let buffer_a = pro_que.create_buffer::<f32>()?;
    let buffer_b = pro_que.create_buffer::<f32>()?;
    let buffer_c = pro_que.create_buffer::<f32>()?;

    buffer_a.write(&m_a).enq()?;
    buffer_b.write(&m_b).enq()?;

    let kernel = pro_que.kernel_builder("matrix_multiply")
        .arg(&buffer_a)
        .arg(&buffer_b)
        .arg(&buffer_c)
        .arg(m_n as u32)
        .build()?;

    let start = Instant::now();

    unsafe { kernel.enq()?; }

    let mut m_c = vec![0.0f32; matrix_size];
    buffer_c.read(&mut m_c).enq()?;

    let duration = start.elapsed();
    println!("Matrix multiplication took: {:?}", duration);

    println!("C[1][1] = {}", m_c[1 * m_n + 1]);

    Ok(())
}

fn main() {
    task().unwrap();
}
```

Which takes around 2s to complete on my machine- very bad. How we can build any AI with such a slow performance? We must optimize it.

### Avoid Global Memory Unless Necessary

Global memory access is extremely slow. How does this hardware fact help? Well, in our previous code, we have,

```c
C[row * N + col] += A[row * N + i] * B[i * N + col];
```

Where `C` is a pointer to the global memory. Each loop, we are writing to the global memory. But it can be avoided by using,

```c
float sum = 0.0f;

for (int i = 0; i < N; i++) {
    sum += A[row * N + i] * B[i * N + col];
}

C[row * N + col] = sum;
```

Local variables are stored in the private memory.

Now let's try again- 950ms, which is less than halved, but still not good.

### Memory Coalescing

We can further reduce the global memory access by a trick called memory coalescing. But before that, we need more hardware details.

For each CU, it has smaller units called either warp (Nvidia) or wavefront (AMD). We use the term wavefront in the future passage.

Each wavefront shares one PC (program counter) register. And within one wavefront, all PE uses the same clock. A typical value of a wavefront is 32.

When fetching from global memory, the work is done in the unit of half-wavefront. And if the PEs in this half-wavefront wants a continuous data chunk (for example, 0-31 or 32-63), it will done in one go, instead of one by one.

What this teaches us? Well, when fetching from global memory, we should ensure that PE fetches should be as continuous as possible, in order to take advantage of this.

Let's check our code with global memory fetching-

```c
sum += A[row * N + i] * B[i * N + col];
```

From `A`, for each half-wavefront, we retrieve a continuous space of data. But for `B`, it is not. How to fix? It's simple, use `B_T` instead.

```c
__kernel void matrix_multiply(
    __global float* A, 
    __global float* B_T,
    __global float* C, 
    const unsigned int N
) {
    int row = get_global_id(0);
    int col = get_global_id(1);
    float sum = 0.0f;
    
    for (int i = 0; i < N; i++) {
        sum += A[row * N + i] * B_T[col * N + i];
    }
    
    C[row * N + col] = sum;
}
```

I'm on Apple Silicon so the architecture is different, thus memory coalescing doesn't yield improvement, but it is important for most GPUs.

### Tweaking Work Group Size

For each architecture, when the work group size matches the PEs in a CU, it typically yield good improvement. However, this is also relative to the global memory fetching and other details. But using matched size can be a good start. Then we can further try to optimize the work group size.

I chose,

```rust
let kernel = pro_que.kernel_builder("matrix_multiply")
    .arg(&buffer_a)
    .arg(&buffer_b)
    .arg(&buffer_c)
    .arg(m_n as u32)
    .local_work_size([8, 8])
    .build()?;
```

And the time taken is 250ms, which is a huge improvement.

### Utilizing Local Memory

This is another trick- previously, we talked about how memory coalescing can reduce global memory fetching. Can we do it further? Yes.

The secret is using local memory. We can make a workgroup fetch global memory only once, store it in the local memory, so that instead of each work item fetching it, each workgroup fetches it only once.

How do we do that? We need to calculate the size of the workgroup (I hardcoded it but you can get it using functions). And prepare the local memory.

```c
const unsigned int TILE_SIZE = 8;

// Thread indices within the workgroup
int row = get_local_id(0);
int col = get_local_id(1);

// Global indices for A and B_T in the entire matrix
int global_row = get_group_id(0) * TILE_SIZE + row;
int global_col = get_group_id(1) * TILE_SIZE + col;

// Local memory to store tiles of A and B_T
__local float local_A[TILE_SIZE][TILE_SIZE];
__local float local_B_T[TILE_SIZE][TILE_SIZE];
```

Then, we ask each PE to load a block of `A` and `B_T` from global memory to local memory. Because of memory coalescing, it is done in one go.

```c
 for (int k = 0; k < N / TILE_SIZE; k++) {
    // Load a block of A from global memory to local memory
    local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)];
    
    // Load a block of B_T (transposed B) from global memory to local memory
    local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)];

    // Synchronize threads to make sure all threads have finished loading data
    barrier(CLK_LOCAL_MEM_FENCE);

    // Perform the multiplication and accumulation for the tile
    for (int i = 0; i < TILE_SIZE; i++) {
        sum += local_A[row][i] * local_B_T[col][i];
    }

    // Synchronize to ensure all threads are done with the current computation before moving to the next
    barrier(CLK_LOCAL_MEM_FENCE);
}
```

We must use `barrier(CLK_LOCAL_MEM_FENCE);` to synchronize workgroup. Otherwise, because PE may store the fetched data into the local memory at different time (fetching is done in one but not storing), the result will be wrong.

Whenever you are about to write to the local memory, a rule of thumb is to do synchronization before and after. Of course, synchronization costs time, so do as few as you can.

Originally, we need `2N` times global fetching. But now, we only need `2N / TILE_SIZE` times global fetching.

```c
__kernel void matrix_multiply(
    __global float* A, 
    __global float* B_T,
    __global float* C, 
    const unsigned int N
) {
    // Tile size (assuming 8x8 workgroup size)
    const unsigned int TILE_SIZE = 8;

    // Thread indices within the workgroup
    int row = get_local_id(0);
    int col = get_local_id(1);

    // Global indices for A and B_T in the entire matrix
    int global_row = get_group_id(0) * TILE_SIZE + row;
    int global_col = get_group_id(1) * TILE_SIZE + col;

    // Local memory to store tiles of A and B_T
    __local float local_A[TILE_SIZE][TILE_SIZE];
    __local float local_B_T[TILE_SIZE][TILE_SIZE];

    float sum = 0.0f;
    
    // Loop through the tiles of A and B_T
    for (int k = 0; k < N / TILE_SIZE; k++) {
        // Load a block of A from global memory to local memory
        local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)];
        
        // Load a block of B_T (transposed B) from global memory to local memory
        local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)];

        // Synchronize threads to make sure all threads have finished loading data
        barrier(CLK_LOCAL_MEM_FENCE);

        // Perform the multiplication and accumulation for the tile
        for (int i = 0; i < TILE_SIZE; i++) {
            sum += local_A[row][i] * local_B_T[col][i];
        }

        // Synchronize to ensure all threads are done with the current computation before moving to the next
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // Write the result to the global memory (C matrix)
    if (global_row < N && global_col < N) {
        C[global_row * N + global_col] = sum;
    }
}
```

Not the time reduced to 60ms! This is great achievements. But please note that to make local memory efficient, you must first do memory coalescing, which, albeit didn't yield much improvement, is still important.

### Using Constant Memory

Constant Memory, well, is obviously faster. When you can use constant memory, use it. Otherwise, use global memory.

To use constant memory, just change the `__global` to `__constant`.

```c
const src_5: &str = r#"
__kernel void matrix_multiply(
    __constant float* A, 
    __constant float* B_T,
    __global float* C, 
    const unsigned int N
) {
    // Tile size (assuming 8x8 workgroup size)
    const unsigned int TILE_SIZE = 8;

    // Thread indices within the workgroup
    int row = get_local_id(0);
    int col = get_local_id(1);

    // Global indices for A and B_T in the entire matrix
    int global_row = get_group_id(0) * TILE_SIZE + row;
    int global_col = get_group_id(1) * TILE_SIZE + col;

    // Local memory to store tiles of A and B_T
    __local float local_A[TILE_SIZE][TILE_SIZE];
    __local float local_B_T[TILE_SIZE][TILE_SIZE];

    float sum = 0.0f;
    
    // Loop through the tiles of A and B_T
    for (int k = 0; k < N / TILE_SIZE; k++) {
        // Load a block of A from global memory to local memory
        local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)];
        
        // Load a block of B_T (transposed B) from global memory to local memory
        local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)];

        // Synchronize threads to make sure all threads have finished loading data
        barrier(CLK_LOCAL_MEM_FENCE);

        // Perform the multiplication and accumulation for the tile
        for (int i = 0; i < TILE_SIZE; i++) {
            sum += local_A[row][i] * local_B_T[col][i];
        }

        // Synchronize to ensure all threads are done with the current computation before moving to the next
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // Write the result to the global memory (C matrix)
    if (global_row < N && global_col < N) {
        C[global_row * N + global_col] = sum;
    }
}
"#;
```

Now the time becomes 55ms. Not very huge improvement, but percentage-wise, it is good.

Now we call it an end for matrix multiplication. There are only a few more trick left, not included in this case.

## Avoid Bank Conflicts

We haven't done with local memories- in reality, local memories are stored in banks- usually the same size as half-wavefront.

The way of storing is lower-bit based. For example, index modulus half-wavefront zero will be stored in bank 0, and index modulus half-wavefront one will be stored in bank 1, etc.

Each bank has its own memory controller, and they are independent. Let's consider the following case,

Assume the half-wavefront size is 4.

- Local Id 0 wants to read from bank 0
- Local Id 1 also wants to read from bank 0
- Local Id 2 wants to read from bank 1
- Local Id 3 also wants to read from bank 1

What would happen? Local Id 0 and Local Id 2 will get their data from the bank 0 and 1, then bank 0 and 1 serves Local Id 1 and 3, which is called, a bank conflict.

How to solve this? The rule is simple- don't read from the same bank at the same time. You can copy an extra local memory to avoid this, in this case.

## Branching is Bad

This is a very simple rule that doesn't need much explanation, but why?

Like we just talked, each wavefront shares a PC register, so let's consider the following code,

```c
int x = 2;
if(mask[get_global_id(0)]) {
    x = 1;
}
else {
    x = 0;
}
```

If a wavefront meets `if(mask[get_global_id(0)])`, and they have the same mask value, then it is fine. But what if not? They share the same PC register that they can't take the same path. This is called a warefront divergence.

Encountered by divergence, the actual execution will be like,

```c
int x = 2;
int x_branch_true = 1;
int x_branch_false = 0;
x = select(mask[get_global_id(0)], x_branch_true, x_branch_false);
```

Yes, both if and else branches will be executed. So it slows down the performance. Thus, we should avoid using `if-else` in our programs.

The above code is simple, but what if there are thousands of lines? It will be disastrous. Thus, put in as few code in the branch as possible.
