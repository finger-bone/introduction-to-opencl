{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to OpenCL These notes presents the basis of OpenCL programming, parallel programming, and GPU architecture. It is a good start for the impatient learner new to parallel programming. The Models in OpenCL Writing OpenCL Programs and OpenCL C Optimizing an OpenCL Program","title":"Introduction to OpenCL"},{"location":"#introduction-to-opencl","text":"These notes presents the basis of OpenCL programming, parallel programming, and GPU architecture. It is a good start for the impatient learner new to parallel programming. The Models in OpenCL Writing OpenCL Programs and OpenCL C Optimizing an OpenCL Program","title":"Introduction to OpenCL"},{"location":"01/","text":"The Models in OpenCL To use OpenCL, we must understand how OpenCL abstracts the real world devices so that we can program with it. Disambiguation The OpenCL standard had a bad choice of words that overlaps other things. For example, they have an abstracted device, but we also talk about real world devices. But device itself already has many meaning in English, and we can refer device to both a graphic card or a full computer. So we need to disambiguate. Anything we talk about in the future content that can both mean a physical thing and an abstracted thing, unless I add physical before it, I will always mean an abstracted thing on the software side. For example, if I say memory, I mean the memory abstracted by the OpenCL. If I say physical memory, I mean the actually physical memory on the GPU. Physical device will be exclusively used for GPU. A complete Computer will not be referred to as a physical device. Computer doesn't need physical prefix because we don't abstract the whole computer. CPU in OpenCL emulates a GPU, so we only focus on GPU architecture. Other type of devices usually have a similar architecture to GPU. Platform and Device OpenCL defines a type of physical devices as a platform, for example, Nvidia GPU, Intel CPU. For a computer with a Xeon CPU and a Nvidia GPU, we have two platforms, one for the Intel CPU and one for the Nvidia GPU. Physical devices will be mapped to devices based on how the manufacturer defines them. Typically, each physical device will be mapped to a single device. For example, a computer with a Xeon CPU and a Nvidia GPU will have two devices, one for the Intel CPU and one for the Nvidia GPU. However, if there are two Nvidia GPUs, they will usually be mapped to two devices. A device can have many context. You can imagine it as something like a logic device, while the real device execute one of the context based on the schedule. A context and a device has identical components, and thus they are confused below for convenience. Usually, in actual programming, the device is solely used for creating a context, then never used again. The Three Major Models The most important part of the OpenCL is how it abstracts physical devices. OpenCL maps heterogeneous devices to same abstracted devices, thus offering a unified programming model. There are three models that are the most important in OpenCL: The Device Model: defines how the device interacts with the host computer. The Memory Model: defines how the memory is managed by the device. The Execution Model: defines how the program is executed on the device. Each model contains copious amount of details. We will introduce the most important ones. However, during this whole series, we will add more and more details to these models. The Device Model The Device Model defines how the device interacts with the host computer. Please note that host computer is a real-world computer, while the device here is an abstracted one. A device (or context) will hold a command queue, which is a, well, queue that accommodates all the commands. The host computer can submit commands to the device. We usually write OpenCL C (a variant of C) programs on the host computer, then dynamically compile them to the assembly of the target device, which will be sent to the command queue. As execution finishes, the result will be written back to the host computer. The Execution Model After the command is sent to the device, it will be executed on the device. The execution model defines how the program is executed on the device. When it comes to parallel programming, we execute the same task many times at the same time. We can tell the device what task it should execute and how many times. We use a three-dimensional tuple to tell the device how many times it should execute the task (x, y, z) , which is x \\times y \\times z times. The reason why we use a tuple instead of a single number will be presented later. Now you need to know that, when we want a device to finish a task, we will distribute a task and a tuple. We call the task a kernel and the tuple a NDRange (N-Dimension range, very bad name). Now you need to know that, instead of a monolithic ALU, cache, other components like CPU, a physical parallel device (usually GPU) has many small cores (aka processing element, PE). Each PE will be abstracted into a work item. The PE are grouped. Within each group, we can share some memory (the local memory, later in the memory model), and we can do synchronization. Each physical group is called a compute unit (CU), and each CU is abstracted into a work group. Kind of confusing, now let's do a pair. A ND Range of tasks will be sent to a physical device . A Work Group of tasks will be sent to a compute unit . A Work Item of tasks will be sent to a processing element . The number of them typically don't match, and thus a PE may execute more than one work items. The ND Range will be splitted into multiple work groups that are usually of the same size as CU (You can add empty task for padding if it doesn't divide). Then, each CU will claim a work group. If there are remaining ones, CU that finishes will claim what's left. Work group size is also a tuple, for example, (128, 128, 128) , which means that each work group will have 128 \\times 128 \\times 128 work items. If we need to execute a ND Range of (1024, 1024, 1024) , we would have 512 work groups. If your device has more than 512 CUs, it will be executed in one go. If not, then it will be executed in batches. A workgroup will always have a continuous chunk that is not breakable. The Memory Model A classical demonstration is as folllows, The picture is originally from this link , by Bj\u00f6rn K\u00f6nig. There are five types of memory. The host memory, which is the memory on the host computer. Usually just the RAM. The global memory of the device. Which is a big chunk of memory that is shared between all the PE. They are big, but slow. It can be accessed by the host. The global constant memory of the device. Which, is global memory, but constant and faster, yet readonly to PE. Can be accessed by the host. The local memory of each work group. It is a small chunk of memory that is only accessible by the PE in the same work group. They are smaller than the global memory, but faster. The private memory of each work item. It is a small chunk of memory that is only accessible by the PE in the same work item. They are the smallest, but fastest. There are many other details such as memory bank, but we will cover them later. Now you just need to know the five types of memory. Please note that we are, again, talking about the abstracted OpenCL device. In reality, take Nvidia GPU as an example, although we say that private memory is held by each work item, a GPU has a monolithic private memory bank that is distributed to each work item. This is also true for the local memory.","title":"The Models in OpenCL"},{"location":"01/#the-models-in-opencl","text":"To use OpenCL, we must understand how OpenCL abstracts the real world devices so that we can program with it.","title":"The Models in OpenCL"},{"location":"01/#disambiguation","text":"The OpenCL standard had a bad choice of words that overlaps other things. For example, they have an abstracted device, but we also talk about real world devices. But device itself already has many meaning in English, and we can refer device to both a graphic card or a full computer. So we need to disambiguate. Anything we talk about in the future content that can both mean a physical thing and an abstracted thing, unless I add physical before it, I will always mean an abstracted thing on the software side. For example, if I say memory, I mean the memory abstracted by the OpenCL. If I say physical memory, I mean the actually physical memory on the GPU. Physical device will be exclusively used for GPU. A complete Computer will not be referred to as a physical device. Computer doesn't need physical prefix because we don't abstract the whole computer. CPU in OpenCL emulates a GPU, so we only focus on GPU architecture. Other type of devices usually have a similar architecture to GPU.","title":"Disambiguation"},{"location":"01/#platform-and-device","text":"OpenCL defines a type of physical devices as a platform, for example, Nvidia GPU, Intel CPU. For a computer with a Xeon CPU and a Nvidia GPU, we have two platforms, one for the Intel CPU and one for the Nvidia GPU. Physical devices will be mapped to devices based on how the manufacturer defines them. Typically, each physical device will be mapped to a single device. For example, a computer with a Xeon CPU and a Nvidia GPU will have two devices, one for the Intel CPU and one for the Nvidia GPU. However, if there are two Nvidia GPUs, they will usually be mapped to two devices. A device can have many context. You can imagine it as something like a logic device, while the real device execute one of the context based on the schedule. A context and a device has identical components, and thus they are confused below for convenience. Usually, in actual programming, the device is solely used for creating a context, then never used again.","title":"Platform and Device"},{"location":"01/#the-three-major-models","text":"The most important part of the OpenCL is how it abstracts physical devices. OpenCL maps heterogeneous devices to same abstracted devices, thus offering a unified programming model. There are three models that are the most important in OpenCL: The Device Model: defines how the device interacts with the host computer. The Memory Model: defines how the memory is managed by the device. The Execution Model: defines how the program is executed on the device. Each model contains copious amount of details. We will introduce the most important ones. However, during this whole series, we will add more and more details to these models.","title":"The Three Major Models"},{"location":"01/#the-device-model","text":"The Device Model defines how the device interacts with the host computer. Please note that host computer is a real-world computer, while the device here is an abstracted one. A device (or context) will hold a command queue, which is a, well, queue that accommodates all the commands. The host computer can submit commands to the device. We usually write OpenCL C (a variant of C) programs on the host computer, then dynamically compile them to the assembly of the target device, which will be sent to the command queue. As execution finishes, the result will be written back to the host computer.","title":"The Device Model"},{"location":"01/#the-execution-model","text":"After the command is sent to the device, it will be executed on the device. The execution model defines how the program is executed on the device. When it comes to parallel programming, we execute the same task many times at the same time. We can tell the device what task it should execute and how many times. We use a three-dimensional tuple to tell the device how many times it should execute the task (x, y, z) , which is x \\times y \\times z times. The reason why we use a tuple instead of a single number will be presented later. Now you need to know that, when we want a device to finish a task, we will distribute a task and a tuple. We call the task a kernel and the tuple a NDRange (N-Dimension range, very bad name). Now you need to know that, instead of a monolithic ALU, cache, other components like CPU, a physical parallel device (usually GPU) has many small cores (aka processing element, PE). Each PE will be abstracted into a work item. The PE are grouped. Within each group, we can share some memory (the local memory, later in the memory model), and we can do synchronization. Each physical group is called a compute unit (CU), and each CU is abstracted into a work group. Kind of confusing, now let's do a pair. A ND Range of tasks will be sent to a physical device . A Work Group of tasks will be sent to a compute unit . A Work Item of tasks will be sent to a processing element . The number of them typically don't match, and thus a PE may execute more than one work items. The ND Range will be splitted into multiple work groups that are usually of the same size as CU (You can add empty task for padding if it doesn't divide). Then, each CU will claim a work group. If there are remaining ones, CU that finishes will claim what's left. Work group size is also a tuple, for example, (128, 128, 128) , which means that each work group will have 128 \\times 128 \\times 128 work items. If we need to execute a ND Range of (1024, 1024, 1024) , we would have 512 work groups. If your device has more than 512 CUs, it will be executed in one go. If not, then it will be executed in batches. A workgroup will always have a continuous chunk that is not breakable.","title":"The Execution Model"},{"location":"01/#the-memory-model","text":"A classical demonstration is as folllows, The picture is originally from this link , by Bj\u00f6rn K\u00f6nig. There are five types of memory. The host memory, which is the memory on the host computer. Usually just the RAM. The global memory of the device. Which is a big chunk of memory that is shared between all the PE. They are big, but slow. It can be accessed by the host. The global constant memory of the device. Which, is global memory, but constant and faster, yet readonly to PE. Can be accessed by the host. The local memory of each work group. It is a small chunk of memory that is only accessible by the PE in the same work group. They are smaller than the global memory, but faster. The private memory of each work item. It is a small chunk of memory that is only accessible by the PE in the same work item. They are the smallest, but fastest. There are many other details such as memory bank, but we will cover them later. Now you just need to know the five types of memory. Please note that we are, again, talking about the abstracted OpenCL device. In reality, take Nvidia GPU as an example, although we say that private memory is held by each work item, a GPU has a monolithic private memory bank that is distributed to each work item. This is also true for the local memory.","title":"The Memory Model"},{"location":"02/","text":"Writing OpenCL Programs and OpenCL C Now we write some OpenCL Programs. OpenCL provides binding for C, and people, of course, have bind that to many languages, including Python, C++, Rust. Because Rust has a very good binding that is extremely easy to use, we will use Rust to write our programs. This series focus on practice, and before start writing any OpenCL code, we need to request a platform, request a device, create a context, create a command queue, compile a program, create buffers, send the program... ocl library provides an excellent ProQue type that eliminates the first few steps, so that we can focus on the essence of parallel programming instead of the trivial. If you want to know how to do that traditionally, or fetch the details of each component, please refer to their example . Bindings for other programming languages typically don't have such a convenient feature, so you'll do the whole venial task above. Now let's start writing some OpenCL programs. The first program is vector sum. Write a OpenCL Program in Rust You'll use cargo to create a new project, then add ocl to the dependency. extern crate ocl; use ocl::ProQue; fn task() -> ocl::Result<()> { let src = r#\" __kernel void add(__global float* buffer, float scalar) { buffer[get_global_id(0)] += scalar; } \"#; let pro_que = ProQue::builder() .src(src) .dims(1 << 4) .build()?; let buffer = pro_que.create_buffer::<f32>()?; let kernel = pro_que.kernel_builder(\"add\") .arg(&buffer) .arg(10.0f32) .build()?; unsafe { kernel.enq()?; } let mut vec = vec![0.0f32; buffer.len()]; buffer.read(&mut vec).enq()?; println!(\"The value at index [{}] is now '{}'!\", 1, vec[1]); Ok(()) } fn main() { task().unwrap(); } The program is simple- we just add 10 to each element of the input vector. Now let's break it down. The OpenCL C Program OpenCL C has special syntax. Now you need to know the following ones, __kernel to declare a function to be a kernel function. Only kernel function can be called from the host. Please note that kernel function can only have void as return value, since, it is called from the host. The way in which the host receive the result is by reading from a buffer. __global to declare a pointer to the global memory. __private to declare a pointer to the private memory. If you don't write anything for a pointer, it is default to __private . get_global_id(dim) is to get the index of the work item that this PE is assigned to. Please distinguish kernel function from kernel. Kernel function is a function that can be later compiled to a kernel (a task to be executed) if we supply it with arguments. Kernel is the actual task that is sent to the device. The pass-by-value acts the same as in C, it is just that the local variables are stored in the private memory. For the following program, __kernel void add(__global float* buffer, float scalar) { buffer[get_global_id(0)] += scalar; } This means that, for the (i, _, _) work item, it will add scalar to the value at the index i of buffer . ProQue ProQue is a utility chimera that has everything we need- we just take the default ProQue , then it will create the platform, device, context, program queue, and give us a command queue with the program. We need to specify our ND range when creating the ProQue , and the program source code. We said that ND Range is a three element tuple. We can use three dim method to set that up in the builder in order. If we don't set a dimension, it is defaulted to one. Buffer We previously claimed that the global and constant global memory of a device are interoperable between itself and the host, which is done by buffer. The buffer is simple- we allocate a chunk of host memory in the host machine, then its content will be sent to another chunk of memory in the global or global constant memory of the device as we execute the kernel. After the execution, mapped chunk of global memory will be written back to the host memory. Thus, if we operate the global memory buffer, we can later get our value on the host machine. By default, create_buffer creates a buffer that is of the same length as the product of the ND Range. If you want more customization, use buffer_builder . let buffer = pro_que.buffer_builder().fill_val(0.0f32).len(1 << 4).build()?; Compile the Kernel Previously, we only put the source code into the ProQue as a string. Now we use kernel_builder to create the kernel. let kernel = pro_que.kernel_builder(\"add\") .arg(&buffer) .arg(10.0f32) .build()?; We first specify the name, then provide the arguments in order. Execute the Kernel Now we just sent the kernel to the device. unsafe { kernel.enq()?; } Please note that this operation is asynchronous, that the code doesn't block and wait for the device to finish. Read the Result Then we send a command to instruct the device to copy the data back to the host machine. let mut vec = vec![0.0f32; buffer.len()]; buffer.read(&mut vec).enq()?; I/O is synchronous, that the code will block and wait for the device to finish. Keep in mind that queue is FIFO, so the device will always finish the kernel, then read the result to the host. OpenCL C Okay, previous part we write our first program. However, there are many OpenCL C things that are different from standard C. You need to remember the following things to write correct OpenCL C, OpenCL C generally follows C99. OpenCL C pointers always have a memory location prefix. If omitted, it is __private . __kernel function must return void. No double pointer. No function pointer. No recursion. struct and union exists and can be used, but may slow down the process. No library. Limited support for inline and static functions. No goto . And there are some extra functions and types that are important, Vector Types OpenCL C has a number of vector types. The most important ones are, <type>2 for 2D vector. This <type> means any native type, for example, float2 , int2 , etc. <type>3 for 3D vector. <type>4 for 4D vector. Work-Item Functions get_global_id(dim) is to get the index of the work item that this PE is assigned to. get_local_id(dim) is to get the offset of the work item that this PE is assigned to within the workgroup. get_group_id(dim) is to get the index of the work group that this PE is assigned to. The dim should be one of zero, one, or two. It may seem hard to understand. Let's use a simple example (all count from zero, that is there are 0th)- we have work items 0 to 15, with each work group has a size of 4, and we have 16 PEs (0 to 15). If we always assume that the distribution is in order. For the 6th PE, it will process the work item 6, and so, the get_global_id(0) is 6 . The get_local_id(0) is 2 , because the 6th PE got the second work item in this workgroup. And for the 6th PE, it's get_group_id(0) is 1 , because the 6th PE processes tasks in the first work group. Still in the previous scenario, if we have work items 0 to 31, in the second go, the get_global_id(0) for the 6th PE will be 22 , because it gets the 22th work item in the second go. The get_local_id(0) is still 2 because, while the get_group_id(0) will be 5 . If the work items are not in order, we just logically re-label all the PE and CUs. In summary, you can think of global_id and group_id as a parameter, where as local_id as a physical parameter that indicates which PE we are using in the CU. This will came into use later. get_global_size(dim) is to get the total length of the ND Range on a dim. get_work_dim() is to get the ND Range dimension. get_local_size(dim) is to get the size of the work group. get_num_work_groups(dim) is to get the number of work groups. Branching Functions if-else is extremely slow in GPU- we will talk about that later. So the following functions are preferred, select(c, a, b) is to select a if c is true, otherwise select b . max(a, b) is to select the maximum of a and b . min(a, b) is to select the minimum of a and b . Atomic Functions These functions are atomic to resolve race condition- we can't use lock here because it's too slow. In the following list, p is of type volatile __global int* . atomic_add(p, val) is to add val to p . atomic_sub(p, val) is to subtract val from p . atomic_xchg(p, val) is to exchange p with val . atomic_inc(p) is to increment p by one. atomic_dec(p) is to decrement p by one. atomic_cmpxchg(p, cmp, val) is to compare p with cmp and exchange p with val if they are equal.","title":"Writing OpenCL Programs and OpenCL C"},{"location":"02/#writing-opencl-programs-and-opencl-c","text":"Now we write some OpenCL Programs. OpenCL provides binding for C, and people, of course, have bind that to many languages, including Python, C++, Rust. Because Rust has a very good binding that is extremely easy to use, we will use Rust to write our programs. This series focus on practice, and before start writing any OpenCL code, we need to request a platform, request a device, create a context, create a command queue, compile a program, create buffers, send the program... ocl library provides an excellent ProQue type that eliminates the first few steps, so that we can focus on the essence of parallel programming instead of the trivial. If you want to know how to do that traditionally, or fetch the details of each component, please refer to their example . Bindings for other programming languages typically don't have such a convenient feature, so you'll do the whole venial task above. Now let's start writing some OpenCL programs. The first program is vector sum.","title":"Writing OpenCL Programs and OpenCL C"},{"location":"02/#write-a-opencl-program-in-rust","text":"You'll use cargo to create a new project, then add ocl to the dependency. extern crate ocl; use ocl::ProQue; fn task() -> ocl::Result<()> { let src = r#\" __kernel void add(__global float* buffer, float scalar) { buffer[get_global_id(0)] += scalar; } \"#; let pro_que = ProQue::builder() .src(src) .dims(1 << 4) .build()?; let buffer = pro_que.create_buffer::<f32>()?; let kernel = pro_que.kernel_builder(\"add\") .arg(&buffer) .arg(10.0f32) .build()?; unsafe { kernel.enq()?; } let mut vec = vec![0.0f32; buffer.len()]; buffer.read(&mut vec).enq()?; println!(\"The value at index [{}] is now '{}'!\", 1, vec[1]); Ok(()) } fn main() { task().unwrap(); } The program is simple- we just add 10 to each element of the input vector. Now let's break it down.","title":"Write a OpenCL Program in Rust"},{"location":"02/#the-opencl-c-program","text":"OpenCL C has special syntax. Now you need to know the following ones, __kernel to declare a function to be a kernel function. Only kernel function can be called from the host. Please note that kernel function can only have void as return value, since, it is called from the host. The way in which the host receive the result is by reading from a buffer. __global to declare a pointer to the global memory. __private to declare a pointer to the private memory. If you don't write anything for a pointer, it is default to __private . get_global_id(dim) is to get the index of the work item that this PE is assigned to. Please distinguish kernel function from kernel. Kernel function is a function that can be later compiled to a kernel (a task to be executed) if we supply it with arguments. Kernel is the actual task that is sent to the device. The pass-by-value acts the same as in C, it is just that the local variables are stored in the private memory. For the following program, __kernel void add(__global float* buffer, float scalar) { buffer[get_global_id(0)] += scalar; } This means that, for the (i, _, _) work item, it will add scalar to the value at the index i of buffer .","title":"The OpenCL C Program"},{"location":"02/#proque","text":"ProQue is a utility chimera that has everything we need- we just take the default ProQue , then it will create the platform, device, context, program queue, and give us a command queue with the program. We need to specify our ND range when creating the ProQue , and the program source code. We said that ND Range is a three element tuple. We can use three dim method to set that up in the builder in order. If we don't set a dimension, it is defaulted to one.","title":"ProQue"},{"location":"02/#buffer","text":"We previously claimed that the global and constant global memory of a device are interoperable between itself and the host, which is done by buffer. The buffer is simple- we allocate a chunk of host memory in the host machine, then its content will be sent to another chunk of memory in the global or global constant memory of the device as we execute the kernel. After the execution, mapped chunk of global memory will be written back to the host memory. Thus, if we operate the global memory buffer, we can later get our value on the host machine. By default, create_buffer creates a buffer that is of the same length as the product of the ND Range. If you want more customization, use buffer_builder . let buffer = pro_que.buffer_builder().fill_val(0.0f32).len(1 << 4).build()?;","title":"Buffer"},{"location":"02/#compile-the-kernel","text":"Previously, we only put the source code into the ProQue as a string. Now we use kernel_builder to create the kernel. let kernel = pro_que.kernel_builder(\"add\") .arg(&buffer) .arg(10.0f32) .build()?; We first specify the name, then provide the arguments in order.","title":"Compile the Kernel"},{"location":"02/#execute-the-kernel","text":"Now we just sent the kernel to the device. unsafe { kernel.enq()?; } Please note that this operation is asynchronous, that the code doesn't block and wait for the device to finish.","title":"Execute the Kernel"},{"location":"02/#read-the-result","text":"Then we send a command to instruct the device to copy the data back to the host machine. let mut vec = vec![0.0f32; buffer.len()]; buffer.read(&mut vec).enq()?; I/O is synchronous, that the code will block and wait for the device to finish. Keep in mind that queue is FIFO, so the device will always finish the kernel, then read the result to the host.","title":"Read the Result"},{"location":"02/#opencl-c","text":"Okay, previous part we write our first program. However, there are many OpenCL C things that are different from standard C. You need to remember the following things to write correct OpenCL C, OpenCL C generally follows C99. OpenCL C pointers always have a memory location prefix. If omitted, it is __private . __kernel function must return void. No double pointer. No function pointer. No recursion. struct and union exists and can be used, but may slow down the process. No library. Limited support for inline and static functions. No goto . And there are some extra functions and types that are important,","title":"OpenCL C"},{"location":"02/#vector-types","text":"OpenCL C has a number of vector types. The most important ones are, <type>2 for 2D vector. This <type> means any native type, for example, float2 , int2 , etc. <type>3 for 3D vector. <type>4 for 4D vector.","title":"Vector Types"},{"location":"02/#work-item-functions","text":"get_global_id(dim) is to get the index of the work item that this PE is assigned to. get_local_id(dim) is to get the offset of the work item that this PE is assigned to within the workgroup. get_group_id(dim) is to get the index of the work group that this PE is assigned to. The dim should be one of zero, one, or two. It may seem hard to understand. Let's use a simple example (all count from zero, that is there are 0th)- we have work items 0 to 15, with each work group has a size of 4, and we have 16 PEs (0 to 15). If we always assume that the distribution is in order. For the 6th PE, it will process the work item 6, and so, the get_global_id(0) is 6 . The get_local_id(0) is 2 , because the 6th PE got the second work item in this workgroup. And for the 6th PE, it's get_group_id(0) is 1 , because the 6th PE processes tasks in the first work group. Still in the previous scenario, if we have work items 0 to 31, in the second go, the get_global_id(0) for the 6th PE will be 22 , because it gets the 22th work item in the second go. The get_local_id(0) is still 2 because, while the get_group_id(0) will be 5 . If the work items are not in order, we just logically re-label all the PE and CUs. In summary, you can think of global_id and group_id as a parameter, where as local_id as a physical parameter that indicates which PE we are using in the CU. This will came into use later. get_global_size(dim) is to get the total length of the ND Range on a dim. get_work_dim() is to get the ND Range dimension. get_local_size(dim) is to get the size of the work group. get_num_work_groups(dim) is to get the number of work groups.","title":"Work-Item Functions"},{"location":"02/#branching-functions","text":"if-else is extremely slow in GPU- we will talk about that later. So the following functions are preferred, select(c, a, b) is to select a if c is true, otherwise select b . max(a, b) is to select the maximum of a and b . min(a, b) is to select the minimum of a and b .","title":"Branching Functions"},{"location":"02/#atomic-functions","text":"These functions are atomic to resolve race condition- we can't use lock here because it's too slow. In the following list, p is of type volatile __global int* . atomic_add(p, val) is to add val to p . atomic_sub(p, val) is to subtract val from p . atomic_xchg(p, val) is to exchange p with val . atomic_inc(p) is to increment p by one. atomic_dec(p) is to decrement p by one. atomic_cmpxchg(p, cmp, val) is to compare p with cmp and exchange p with val if they are equal.","title":"Atomic Functions"},{"location":"03/","text":"Optimizing an OpenCL Program Actually, after learning the previous two parts, you have already exhausted the most of the OpenCL features, and you can write many parallel programs. However, you may make tons of mistakes that make your program run slower. This chapter introduce common mistakes and how to fix them. Optimization is usually related to the hardware details. Matrix Multiplication Most of them can be demonstrated with a simple case- matrix multiplication. We first write the simplest program, extern crate ocl; use ocl::{ProQue}; use std::time::Instant; const src_1: &str = r#\" __kernel void matrix_multiply(__global float* A, __global float* B, __global float* C, const unsigned int N) { int row = get_global_id(0); int col = get_global_id(1); for (int i = 0; i < N; i++) { C[row * N + col] += A[row * N + i] * B[i * N + col]; } } \"#; fn task() -> ocl::Result<()> { let m_n = 2048; let matrix_size = m_n * m_n; let mut m_a = vec![0.0f32; matrix_size]; let mut m_b = vec![0.0f32; matrix_size]; for i in 0..m_n { for j in 0..m_n { m_a[i * m_n + j] = (i as f32) + (j as f32); m_b[i * m_n + j] = (i as f32) - (j as f32); } } let pro_que = ProQue::builder() .src(src_1) .dims([m_n, m_n]) .build()?; let buffer_a = pro_que.create_buffer::<f32>()?; let buffer_b = pro_que.create_buffer::<f32>()?; let buffer_c = pro_que.create_buffer::<f32>()?; buffer_a.write(&m_a).enq()?; buffer_b.write(&m_b).enq()?; let kernel = pro_que.kernel_builder(\"matrix_multiply\") .arg(&buffer_a) .arg(&buffer_b) .arg(&buffer_c) .arg(m_n as u32) .build()?; let start = Instant::now(); unsafe { kernel.enq()?; } let mut m_c = vec![0.0f32; matrix_size]; buffer_c.read(&mut m_c).enq()?; let duration = start.elapsed(); println!(\"Matrix multiplication took: {:?}\", duration); println!(\"C[1][1] = {}\", m_c[1 * m_n + 1]); Ok(()) } fn main() { task().unwrap(); } Which takes around 2s to complete on my machine- very bad. How we can build any AI with such a slow performance? We must optimize it. Avoid Global Memory Unless Necessary Global memory access is extremely slow. How does this hardware fact help? Well, in our previous code, we have, C[row * N + col] += A[row * N + i] * B[i * N + col]; Where C is a pointer to the global memory. Each loop, we are writing to the global memory. But it can be avoided by using, float sum = 0.0f; for (int i = 0; i < N; i++) { sum += A[row * N + i] * B[i * N + col]; } C[row * N + col] = sum; Local variables are stored in the private memory. Now let's try again- 950ms, which is less than halved, but still not good. Memory Coalescing We can further reduce the global memory access by a trick called memory coalescing. But before that, we need more hardware details. For each CU, it has smaller units called either warp (Nvidia) or wavefront (AMD). We use the term wavefront in the future passage. Each wavefront shares one PC (program counter) register. And within one wavefront, all PE uses the same clock. A typical value of a wavefront is 32. When fetching from global memory, the work is done in the unit of half-wavefront. And if the PEs in this half-wavefront wants a continuous data chunk (for example, 0-31 or 32-63), it will done in one go, instead of one by one. What this teaches us? Well, when fetching from global memory, we should ensure that PE fetches should be as continuous as possible, in order to take advantage of this. Let's check our code with global memory fetching- sum += A[row * N + i] * B[i * N + col]; From A , for each half-wavefront, we retrieve a continuous space of data. But for B , it is not. How to fix? It's simple, use B_T instead. __kernel void matrix_multiply( __global float* A, __global float* B_T, __global float* C, const unsigned int N ) { int row = get_global_id(0); int col = get_global_id(1); float sum = 0.0f; for (int i = 0; i < N; i++) { sum += A[row * N + i] * B_T[col * N + i]; } C[row * N + col] = sum; } I'm on Apple Silicon so the architecture is different, thus memory coalescing doesn't yield improvement, but it is important for most GPUs. Tweaking Work Group Size For each architecture, when the work group size matches the PEs in a CU, it typically yield good improvement. However, this is also relative to the global memory fetching and other details. But using matched size can be a good start. Then we can further try to optimize the work group size. I chose, let kernel = pro_que.kernel_builder(\"matrix_multiply\") .arg(&buffer_a) .arg(&buffer_b) .arg(&buffer_c) .arg(m_n as u32) .local_work_size([8, 8]) .build()?; And the time taken is 250ms, which is a huge improvement. We will call it an end here for matrix multiplication. Utilizing Local Memory This is another trick- previously, we talked about how memory coalescing can reduce global memory fetching. Can we do it further? Yes. The secret is using local memory. We can make a workgroup fetch global memory only once, store it in the local memory, so that instead of each work item fetching it, each workgroup fetches it only once. How do we do that? We need to calculate the size of the workgroup (I hardcoded it but you can get it using functions). And prepare the local memory. const unsigned int TILE_SIZE = 8; // Thread indices within the workgroup int row = get_local_id(0); int col = get_local_id(1); // Global indices for A and B_T in the entire matrix int global_row = get_group_id(0) * TILE_SIZE + row; int global_col = get_group_id(1) * TILE_SIZE + col; // Local memory to store tiles of A and B_T __local float local_A[TILE_SIZE][TILE_SIZE]; __local float local_B_T[TILE_SIZE][TILE_SIZE]; Then, we ask each PE to load a block of A and B_T from global memory to local memory. Because of memory coalescing, it is done in one go. for (int k = 0; k < N / TILE_SIZE; k++) { // Load a block of A from global memory to local memory local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)]; // Load a block of B_T (transposed B) from global memory to local memory local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)]; // Synchronize threads to make sure all threads have finished loading data barrier(CLK_LOCAL_MEM_FENCE); // Perform the multiplication and accumulation for the tile for (int i = 0; i < TILE_SIZE; i++) { sum += local_A[row][i] * local_B_T[col][i]; } // Synchronize to ensure all threads are done with the current computation before moving to the next barrier(CLK_LOCAL_MEM_FENCE); } We must use barrier(CLK_LOCAL_MEM_FENCE); to synchronize workgroup. Otherwise, because PE may store the fetched data into the local memory at different time (fetching is done in one but not storing), the result will be wrong. Whenever you are about to write to the local memory, a rule of thumb is to do synchronization before and after. Of course, synchronization costs time, so do as few as you can. Originally, we need 2N times global fetching. But now, we only need 2N / TILE_SIZE times global fetching. __kernel void matrix_multiply( __global float* A, __global float* B_T, __global float* C, const unsigned int N ) { // Tile size (assuming 8x8 workgroup size) const unsigned int TILE_SIZE = 8; // Thread indices within the workgroup int row = get_local_id(0); int col = get_local_id(1); // Global indices for A and B_T in the entire matrix int global_row = get_group_id(0) * TILE_SIZE + row; int global_col = get_group_id(1) * TILE_SIZE + col; // Local memory to store tiles of A and B_T __local float local_A[TILE_SIZE][TILE_SIZE]; __local float local_B_T[TILE_SIZE][TILE_SIZE]; float sum = 0.0f; // Loop through the tiles of A and B_T for (int k = 0; k < N / TILE_SIZE; k++) { // Load a block of A from global memory to local memory local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)]; // Load a block of B_T (transposed B) from global memory to local memory local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)]; // Synchronize threads to make sure all threads have finished loading data barrier(CLK_LOCAL_MEM_FENCE); // Perform the multiplication and accumulation for the tile for (int i = 0; i < TILE_SIZE; i++) { sum += local_A[row][i] * local_B_T[col][i]; } // Synchronize to ensure all threads are done with the current computation before moving to the next barrier(CLK_LOCAL_MEM_FENCE); } // Write the result to the global memory (C matrix) if (global_row < N && global_col < N) { C[global_row * N + global_col] = sum; } } Not the time reduced to 60ms! This is great achievements. But please note that to make local memory efficient, you must first do memory coalescing, which, albeit didn't yield much improvement, is still important. Use Constant Memory Constant Memory, well, is obviously faster. When you can use constant memory, use it. Otherwise, use global memory. To use constant memory, just change the __global to __constant . const src_5: &str = r#\" __kernel void matrix_multiply( __constant float* A, __constant float* B_T, __global float* C, const unsigned int N ) { // Tile size (assuming 8x8 workgroup size) const unsigned int TILE_SIZE = 8; // Thread indices within the workgroup int row = get_local_id(0); int col = get_local_id(1); // Global indices for A and B_T in the entire matrix int global_row = get_group_id(0) * TILE_SIZE + row; int global_col = get_group_id(1) * TILE_SIZE + col; // Local memory to store tiles of A and B_T __local float local_A[TILE_SIZE][TILE_SIZE]; __local float local_B_T[TILE_SIZE][TILE_SIZE]; float sum = 0.0f; // Loop through the tiles of A and B_T for (int k = 0; k < N / TILE_SIZE; k++) { // Load a block of A from global memory to local memory local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)]; // Load a block of B_T (transposed B) from global memory to local memory local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)]; // Synchronize threads to make sure all threads have finished loading data barrier(CLK_LOCAL_MEM_FENCE); // Perform the multiplication and accumulation for the tile for (int i = 0; i < TILE_SIZE; i++) { sum += local_A[row][i] * local_B_T[col][i]; } // Synchronize to ensure all threads are done with the current computation before moving to the next barrier(CLK_LOCAL_MEM_FENCE); } // Write the result to the global memory (C matrix) if (global_row < N && global_col < N) { C[global_row * N + global_col] = sum; } } \"#; Now the time becomes 55ms. Not very huge improvement, but percentage-wise, it is good. Now we call it an end for matrix multiplication. There are only a few more trick left, not included in this case. Avoid Bank Conflicts We haven't done with local memories- in reality, local memories are stored in banks- usually the same size as half-wavefront. The way of storing is lower-bit based. For example, index modulus half-wavefront zero will be stored in bank 0, and index modulus half-wavefront one will be stored in bank 1, etc. Each bank has its own memory controller, and they are independent. Let's consider the following case, Assume the half-wavefront size is 4. Local Id 0 wants to read from bank 0 Local Id 1 also wants to read from bank 0 Local Id 2 wants to read from bank 1 Local Id 3 also wants to read from bank 1 What would happen? Local Id 0 and Local Id 2 will get their data from the bank 0 and 1, then bank 0 and 1 serves Local Id 1 and 3, which is called, a bank conflict. How to solve this? The rule is simple- don't read from the same bank at the same time. You can copy an extra local memory to avoid this, in this case. Branching is Bad This is a very simple rule that doesn't need much explanation, but why? Like we just talked, each wavefront shares a PC register, so let's consider the following code, int x = 2; if(mask[get_global_id(0)]) { x = 1; } else { x = 0; } If a wavefront meets if(mask[get_global_id(0)]) , and they have the same mask value, then it is fine. But what if not? They share the same PC register that they can't take the same path. This is called a warefront divergence. Encountered by divergence, the actual execution will be like, int x = 2; int x_branch_true = 1; int x_branch_false = 0; x = select(mask[get_global_id(0)], x_branch_true, x_branch_false); Yes, both if and else branches will be executed. So it slows down the performance. Thus, we should avoid using if-else in our programs. The above code is simple, but what if there are thousands of lines? It will be disastrous. Thus, put in as few code in the branch as possible.","title":"Optimizing an OpenCL Program"},{"location":"03/#optimizing-an-opencl-program","text":"Actually, after learning the previous two parts, you have already exhausted the most of the OpenCL features, and you can write many parallel programs. However, you may make tons of mistakes that make your program run slower. This chapter introduce common mistakes and how to fix them. Optimization is usually related to the hardware details.","title":"Optimizing an OpenCL Program"},{"location":"03/#matrix-multiplication","text":"Most of them can be demonstrated with a simple case- matrix multiplication. We first write the simplest program, extern crate ocl; use ocl::{ProQue}; use std::time::Instant; const src_1: &str = r#\" __kernel void matrix_multiply(__global float* A, __global float* B, __global float* C, const unsigned int N) { int row = get_global_id(0); int col = get_global_id(1); for (int i = 0; i < N; i++) { C[row * N + col] += A[row * N + i] * B[i * N + col]; } } \"#; fn task() -> ocl::Result<()> { let m_n = 2048; let matrix_size = m_n * m_n; let mut m_a = vec![0.0f32; matrix_size]; let mut m_b = vec![0.0f32; matrix_size]; for i in 0..m_n { for j in 0..m_n { m_a[i * m_n + j] = (i as f32) + (j as f32); m_b[i * m_n + j] = (i as f32) - (j as f32); } } let pro_que = ProQue::builder() .src(src_1) .dims([m_n, m_n]) .build()?; let buffer_a = pro_que.create_buffer::<f32>()?; let buffer_b = pro_que.create_buffer::<f32>()?; let buffer_c = pro_que.create_buffer::<f32>()?; buffer_a.write(&m_a).enq()?; buffer_b.write(&m_b).enq()?; let kernel = pro_que.kernel_builder(\"matrix_multiply\") .arg(&buffer_a) .arg(&buffer_b) .arg(&buffer_c) .arg(m_n as u32) .build()?; let start = Instant::now(); unsafe { kernel.enq()?; } let mut m_c = vec![0.0f32; matrix_size]; buffer_c.read(&mut m_c).enq()?; let duration = start.elapsed(); println!(\"Matrix multiplication took: {:?}\", duration); println!(\"C[1][1] = {}\", m_c[1 * m_n + 1]); Ok(()) } fn main() { task().unwrap(); } Which takes around 2s to complete on my machine- very bad. How we can build any AI with such a slow performance? We must optimize it.","title":"Matrix Multiplication"},{"location":"03/#avoid-global-memory-unless-necessary","text":"Global memory access is extremely slow. How does this hardware fact help? Well, in our previous code, we have, C[row * N + col] += A[row * N + i] * B[i * N + col]; Where C is a pointer to the global memory. Each loop, we are writing to the global memory. But it can be avoided by using, float sum = 0.0f; for (int i = 0; i < N; i++) { sum += A[row * N + i] * B[i * N + col]; } C[row * N + col] = sum; Local variables are stored in the private memory. Now let's try again- 950ms, which is less than halved, but still not good.","title":"Avoid Global Memory Unless Necessary"},{"location":"03/#memory-coalescing","text":"We can further reduce the global memory access by a trick called memory coalescing. But before that, we need more hardware details. For each CU, it has smaller units called either warp (Nvidia) or wavefront (AMD). We use the term wavefront in the future passage. Each wavefront shares one PC (program counter) register. And within one wavefront, all PE uses the same clock. A typical value of a wavefront is 32. When fetching from global memory, the work is done in the unit of half-wavefront. And if the PEs in this half-wavefront wants a continuous data chunk (for example, 0-31 or 32-63), it will done in one go, instead of one by one. What this teaches us? Well, when fetching from global memory, we should ensure that PE fetches should be as continuous as possible, in order to take advantage of this. Let's check our code with global memory fetching- sum += A[row * N + i] * B[i * N + col]; From A , for each half-wavefront, we retrieve a continuous space of data. But for B , it is not. How to fix? It's simple, use B_T instead. __kernel void matrix_multiply( __global float* A, __global float* B_T, __global float* C, const unsigned int N ) { int row = get_global_id(0); int col = get_global_id(1); float sum = 0.0f; for (int i = 0; i < N; i++) { sum += A[row * N + i] * B_T[col * N + i]; } C[row * N + col] = sum; } I'm on Apple Silicon so the architecture is different, thus memory coalescing doesn't yield improvement, but it is important for most GPUs.","title":"Memory Coalescing"},{"location":"03/#tweaking-work-group-size","text":"For each architecture, when the work group size matches the PEs in a CU, it typically yield good improvement. However, this is also relative to the global memory fetching and other details. But using matched size can be a good start. Then we can further try to optimize the work group size. I chose, let kernel = pro_que.kernel_builder(\"matrix_multiply\") .arg(&buffer_a) .arg(&buffer_b) .arg(&buffer_c) .arg(m_n as u32) .local_work_size([8, 8]) .build()?; And the time taken is 250ms, which is a huge improvement. We will call it an end here for matrix multiplication.","title":"Tweaking Work Group Size"},{"location":"03/#utilizing-local-memory","text":"This is another trick- previously, we talked about how memory coalescing can reduce global memory fetching. Can we do it further? Yes. The secret is using local memory. We can make a workgroup fetch global memory only once, store it in the local memory, so that instead of each work item fetching it, each workgroup fetches it only once. How do we do that? We need to calculate the size of the workgroup (I hardcoded it but you can get it using functions). And prepare the local memory. const unsigned int TILE_SIZE = 8; // Thread indices within the workgroup int row = get_local_id(0); int col = get_local_id(1); // Global indices for A and B_T in the entire matrix int global_row = get_group_id(0) * TILE_SIZE + row; int global_col = get_group_id(1) * TILE_SIZE + col; // Local memory to store tiles of A and B_T __local float local_A[TILE_SIZE][TILE_SIZE]; __local float local_B_T[TILE_SIZE][TILE_SIZE]; Then, we ask each PE to load a block of A and B_T from global memory to local memory. Because of memory coalescing, it is done in one go. for (int k = 0; k < N / TILE_SIZE; k++) { // Load a block of A from global memory to local memory local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)]; // Load a block of B_T (transposed B) from global memory to local memory local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)]; // Synchronize threads to make sure all threads have finished loading data barrier(CLK_LOCAL_MEM_FENCE); // Perform the multiplication and accumulation for the tile for (int i = 0; i < TILE_SIZE; i++) { sum += local_A[row][i] * local_B_T[col][i]; } // Synchronize to ensure all threads are done with the current computation before moving to the next barrier(CLK_LOCAL_MEM_FENCE); } We must use barrier(CLK_LOCAL_MEM_FENCE); to synchronize workgroup. Otherwise, because PE may store the fetched data into the local memory at different time (fetching is done in one but not storing), the result will be wrong. Whenever you are about to write to the local memory, a rule of thumb is to do synchronization before and after. Of course, synchronization costs time, so do as few as you can. Originally, we need 2N times global fetching. But now, we only need 2N / TILE_SIZE times global fetching. __kernel void matrix_multiply( __global float* A, __global float* B_T, __global float* C, const unsigned int N ) { // Tile size (assuming 8x8 workgroup size) const unsigned int TILE_SIZE = 8; // Thread indices within the workgroup int row = get_local_id(0); int col = get_local_id(1); // Global indices for A and B_T in the entire matrix int global_row = get_group_id(0) * TILE_SIZE + row; int global_col = get_group_id(1) * TILE_SIZE + col; // Local memory to store tiles of A and B_T __local float local_A[TILE_SIZE][TILE_SIZE]; __local float local_B_T[TILE_SIZE][TILE_SIZE]; float sum = 0.0f; // Loop through the tiles of A and B_T for (int k = 0; k < N / TILE_SIZE; k++) { // Load a block of A from global memory to local memory local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)]; // Load a block of B_T (transposed B) from global memory to local memory local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)]; // Synchronize threads to make sure all threads have finished loading data barrier(CLK_LOCAL_MEM_FENCE); // Perform the multiplication and accumulation for the tile for (int i = 0; i < TILE_SIZE; i++) { sum += local_A[row][i] * local_B_T[col][i]; } // Synchronize to ensure all threads are done with the current computation before moving to the next barrier(CLK_LOCAL_MEM_FENCE); } // Write the result to the global memory (C matrix) if (global_row < N && global_col < N) { C[global_row * N + global_col] = sum; } } Not the time reduced to 60ms! This is great achievements. But please note that to make local memory efficient, you must first do memory coalescing, which, albeit didn't yield much improvement, is still important.","title":"Utilizing Local Memory"},{"location":"03/#use-constant-memory","text":"Constant Memory, well, is obviously faster. When you can use constant memory, use it. Otherwise, use global memory. To use constant memory, just change the __global to __constant . const src_5: &str = r#\" __kernel void matrix_multiply( __constant float* A, __constant float* B_T, __global float* C, const unsigned int N ) { // Tile size (assuming 8x8 workgroup size) const unsigned int TILE_SIZE = 8; // Thread indices within the workgroup int row = get_local_id(0); int col = get_local_id(1); // Global indices for A and B_T in the entire matrix int global_row = get_group_id(0) * TILE_SIZE + row; int global_col = get_group_id(1) * TILE_SIZE + col; // Local memory to store tiles of A and B_T __local float local_A[TILE_SIZE][TILE_SIZE]; __local float local_B_T[TILE_SIZE][TILE_SIZE]; float sum = 0.0f; // Loop through the tiles of A and B_T for (int k = 0; k < N / TILE_SIZE; k++) { // Load a block of A from global memory to local memory local_A[row][col] = A[global_row * N + (k * TILE_SIZE + col)]; // Load a block of B_T (transposed B) from global memory to local memory local_B_T[row][col] = B_T[global_col * N + (k * TILE_SIZE + row)]; // Synchronize threads to make sure all threads have finished loading data barrier(CLK_LOCAL_MEM_FENCE); // Perform the multiplication and accumulation for the tile for (int i = 0; i < TILE_SIZE; i++) { sum += local_A[row][i] * local_B_T[col][i]; } // Synchronize to ensure all threads are done with the current computation before moving to the next barrier(CLK_LOCAL_MEM_FENCE); } // Write the result to the global memory (C matrix) if (global_row < N && global_col < N) { C[global_row * N + global_col] = sum; } } \"#; Now the time becomes 55ms. Not very huge improvement, but percentage-wise, it is good. Now we call it an end for matrix multiplication. There are only a few more trick left, not included in this case.","title":"Use Constant Memory"},{"location":"03/#avoid-bank-conflicts","text":"We haven't done with local memories- in reality, local memories are stored in banks- usually the same size as half-wavefront. The way of storing is lower-bit based. For example, index modulus half-wavefront zero will be stored in bank 0, and index modulus half-wavefront one will be stored in bank 1, etc. Each bank has its own memory controller, and they are independent. Let's consider the following case, Assume the half-wavefront size is 4. Local Id 0 wants to read from bank 0 Local Id 1 also wants to read from bank 0 Local Id 2 wants to read from bank 1 Local Id 3 also wants to read from bank 1 What would happen? Local Id 0 and Local Id 2 will get their data from the bank 0 and 1, then bank 0 and 1 serves Local Id 1 and 3, which is called, a bank conflict. How to solve this? The rule is simple- don't read from the same bank at the same time. You can copy an extra local memory to avoid this, in this case.","title":"Avoid Bank Conflicts"},{"location":"03/#branching-is-bad","text":"This is a very simple rule that doesn't need much explanation, but why? Like we just talked, each wavefront shares a PC register, so let's consider the following code, int x = 2; if(mask[get_global_id(0)]) { x = 1; } else { x = 0; } If a wavefront meets if(mask[get_global_id(0)]) , and they have the same mask value, then it is fine. But what if not? They share the same PC register that they can't take the same path. This is called a warefront divergence. Encountered by divergence, the actual execution will be like, int x = 2; int x_branch_true = 1; int x_branch_false = 0; x = select(mask[get_global_id(0)], x_branch_true, x_branch_false); Yes, both if and else branches will be executed. So it slows down the performance. Thus, we should avoid using if-else in our programs. The above code is simple, but what if there are thousands of lines? It will be disastrous. Thus, put in as few code in the branch as possible.","title":"Branching is Bad"}]}